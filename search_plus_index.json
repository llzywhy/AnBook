{"./":{"url":"./","title":"简介","keywords":"","body":"AnBook 文档访问链接 https://llzywhy.github.io/AnBook/ 目的 建立 Android 音视频知识库 探索一条清晰的系统的学习音视频的路径 定位 Android 音视频，多人协作​ 目标人群​ 多人协作 集多人智慧于一体 三个臭皮匠顶一个诸葛亮 健壮性强 发展 维护 利于个人创作，发挥个人的特长 撰写 所有文档均使用 Markdown 撰写 原则 表达必须简单易懂，不费脑力 用词必须准确，避免误解 结构清晰简单有逻辑性 整体框架大于关键点远大于细节，且整体和细节必须分开 能用一句话表达的，绝不用两句话 牵扯的东西越少越好，越独立越好 二八原则。用 20% 的知识撬动剩下的 80% 的知识 思考 一个东西往往会有多个视图，多个角度 一个东西往往会有历史，且不完美 一个东西一般可分内和外两个部分，由外到内的学习过程比较符合正常思维过程 文档需要不断维护 学习和开发的平台和环境，使用案例或者demo 先用起来，操作起来，可以摸到边界，看到范围 词条一句话简单解释 递进式层级文档 顺序要符合逻辑和理解 官方文档和好的文章支撑 搭建框架，占领关键核心绕不过去的部分 红线 避免摆细节 避免不讲逻辑 开发 / 贡献环境 node 环境 gitbook 已很久没有更新，最新的 node 版本并不适配，需要特定的 node 版本。 建议使用 nvm 管理 node 版本。 以下版本测试没有问题, 使用如下的版本即可。 $ node -v v13.14.0 $ npm -v 6.14.14 gitbook 安装 $ npm install -g gitbook-cli $ gitbook --version CLI version: 2.3.2 GitBook version: 3.2.3 仓库下载 git clone https://github.com/llzywhy/AnBook.git cd AnBook # 进入到仓库根目录中 npm install # 安装全部依赖 本地调试 cd AnBook # 进入到仓库根目录中 npm run serve 一切 OK 即可通过如下链接访问本地文档：http://localhost:4000/ 本地编译 cd AnBook # 进入到仓库根目录中 npm run build 根目录下的 _book/ 目录即是生成的静态网站文件 修改文档 所有的文档都存放在 doc/ 目录下，并合理分类找到相应的文档位置，打开编辑修改即可 新建文档 修改 SUMMARY.md 文件，在相应合适的位置添加条目然后执行如下命令，即可在相应的位置生成文件 npm run init push 修改 修改完并本地调试 OK 之后，即可按照 git 流程 push 相应的修改到 main branch push 到 main 的命令： cd AnBook # 进入到仓库根目录中 git push origin main github merge 后，会自动触发 workflow, 自动更新文档 插件安装和卸载 安装: cd AnBook # 进入到仓库根目录中 npm install gitbook-plugin-theme-lou # gitbook-plugin-theme-lou 是插件名，需要自行修改 卸载： cd AnBook # 进入到仓库根目录中 npm uninstall gitbook-plugin-theme-lou # gitbook-plugin-theme-lou 是插件名，需要自行修改 创建插件 patch 有时候需要修改插件代码来满足我们的需求，比如修改 gitbook 主题修改插件代码之后,需要生成一个 patch 来把修改保存下来 创建 patch 的命令： cd AnBook # 进入到仓库根目录中 npx patch-package gitbook-plugin-theme-lou # gitbook-plugin-theme-lou 是插件名，需要自行修改 生成的 patch 文件存放在仓库根目录的 patches/ 目录中 "},"doc/andriod/android_overview.html":{"url":"doc/andriod/android_overview.html","title":"Android 概述","keywords":"","body":"Android 概述 Android 源码在线阅读 https://cs.android.com/android/platform/superproject ​​Android 操作系统文档 https://source.android.com/docs Android 应用开发文档 https://developer.android.com/about Android 系统框架 Player ExoPlayer​NuPlayer​MediaPlayerVLC 音视频处理框架/Android 多媒体框架 FFmpegGSteamerOpenMaxCodec2MediaCodecV4L2tinyalsa/alsa 音视频编解码 video H264H265 audio AACOpus 音视频传输协议 WebRTCRTPRTCPHLSHTTPTCPIPRTSPIGMP 投屏协议 DLNAMiracastAirPlay 其他 gitgerritJinkensopengrokRDKYoctobuildrootsystrace​​dumpsysNinja​​bugreport "},"doc/andriod/android_system_architecture.html":{"url":"doc/andriod/android_system_architecture.html","title":"Android 系统架构","keywords":"","body":"Android 系统架构 本篇文章目标 了解 Android 系统架构 系统架构图 Android 是一种基于 Linux 的开放源代码软件栈，为各类设备和机型而创建。下图所示为 Android 平台的主要组件。 基于 linux, 软件栈 Linux 内核 Android 平台的基础是 Linux 内核。例如，Android Runtime (ART) 依靠 Linux 内核来执行底层功能例如，线程和低层内存管理。 linux 内核 硬件抽象层 (HAL) 硬件抽象层 (HAL) 提供标准界面，向更高级别的 Java API 框架显示设备硬件功能。HAL 包含多个库模块，其中每个模块都为特定类型的硬件组件实现一个界面，例如相机或蓝牙模块。当框架 API 要求访问设备硬件时，Android 系统将为该硬件组件加载库模块。 HAL, 提供标准 API Android Runtime 对于运行 Android 5.0（API 级别 21）或更高版本的设备，每个应用都在其自己的进程中运行，并且有其自己的 Android Runtime (ART) 实例。 ART 编写为通过执行 DEX 文件在低内存设备上运行多个虚拟机，DEX 文件是一种专为 Android 设计的字节码格式，经过优化，使用的内存很少。编译工具链（例如 Jack）将 Java 源代码编译为 DEX 字节码，使其可在 Android 平台上运行。 ART 的部分主要功能包括： 预先 (AOT) 和即时 (JIT) 编译 优化的垃圾回收 (GC) 在 Android 9（API 级别 28）及更高版本的系统中，支持将应用软件包中的 Dalvik Executable 格式 (DEX) 文件转换为更紧凑的机器代码。 更好的调试支持，包括专用采样分析器、详细的诊断异常和崩溃报告，并且能够设置观察点以监控特定字段 在 Android 版本 5.0（API 级别 21）之前，Dalvik 是 Android Runtime。如果您的应用在 ART 上运行效果很好，那么它应该也可在 Dalvik 上运行，但反过来不一定。 Android 还包含一套核心运行时库，可提供 Java API 框架所使用的 Java 编程语言中的大部分功能，包括一些 Java 8 语言功能。 ART: Android Runtime, 运行时 原生 C/C++ 库 许多核心 Android 系统组件和服务（例如 ART 和 HAL）构建自原生代码，需要以 C 和 C++ 编写的原生库。 Android 平台提供 Java 框架 API 以向应用显示其中部分原生库的功能。例如，您可以通过 Android 框架的 Java OpenGL API 访问 OpenGL ES，以支持在应用中绘制和操作 2D 和 3D 图形。 如果开发的是需要 C 或 C++ 代码的应用，可以使用 Android NDK 直接从原生代码访问某些原生平台库。 为了使用原生 C/C++ 库 Java API 框架 您可通过以 Java 语言编写的 API, 使用 Android OS 的整个功能集。这些 API 形成创建 Android 应用所需的构建块，它们可简化核心模块化系统组件和服务的重复使用，包括以下组件和服务： 丰富、可扩展的视图系统，可用以构建应用的 UI，包括列表、网格、文本框、按钮甚至可嵌入的网络浏览器 资源管理器，用于访问非代码资源，例如本地化的字符串、图形和布局文件 通知管理器，可让所有应用在状态栏中显示自定义提醒 Activity 管理器，用于管理应用的生命周期，提供常见的导航返回栈 内容提供程序，可让应用访问其他应用（例如“联系人”应用）中的数据或者共享其自己的数据 开发者可以完全访问 Android 系统应用使用的框架 API。 JAVA API 系统应用 Android 随附一套用于电子邮件、短信、日历、互联网浏览和联系人等的核心应用。 平台随附的应用与用户可以选择安装的应用一样，没有特殊状态。因此第三方应用可成为用户的默认网络浏览器、短信 Messenger 甚至默认键盘（有一些例外，例如系统的“设置”应用）。 系统应用可用作用户的应用，以及提供开发者可从其自己的应用访问的主要功能。例如，如果您的应用要发短信，您无需自己构建该功能，可以改为调用已安装的短信应用向您指定的接收者发送消息。 应用 系统启动架构图 Android 系统启动过程由上图从下往上的一个过程是由 Boot Loader 引导开机，然后依次进入 Kernel -> Native -> Framework -> App 参考资料 https://developer.android.com/guide/platform?hl=zh-cn "},"doc/andriod/Binder.html":{"url":"doc/andriod/Binder.html","title":"Binder","keywords":"","body":"Binder 本篇文章目标 理解 binder 熟悉 binder 在 Android 的使用 了解 binder 的通信过程 binder 是什么 Binder 是 Android 系统进程间通信​（IPC）方式之一，它是基于开源的 OpenBinder 实现的Android 系统的许多服务都是基于 Binder，所以理解 Binder 对于理解服务的流程有很大的帮助 关键点：进程间通信 实现binder的理由 1. 通信方式 Android 系统有各种复杂的服务，为了应对各种复杂的服务之间的通信，需要实现一种基于Client-Server的通信方式。而现有的消息队列/共享内存/信号量/socket 等IPC都不符合要求。 关键点：Client-Server 2. 传输性能 各种IPC方式数据拷贝次数 IPC 数据拷贝次数 共享内存 0 Binder 1 Socket/管道/消息队列 2 关键点：只拷贝1次 3. 安全性考虑 Android作为一个开放式，拥有众多开发者的的平台，应用程序的来源广泛，确保智能终端的安全是非常重要的。终端用户不希望从网上下载的程序在不知情的情况下偷窥隐私数据，连接无线网络，长期操作底层设备导致电池很快耗尽等等。传统IPC没有任何安全措施，完全依赖上层协议来确保。首先传统IPC的接收方无法获得对方进程可靠的UID/PID（用户ID/进程ID），从而无法鉴别对方身份。 Android为每个安装好的应用程序分配了自己的UID，故进程的UID是鉴别进程身份的重要标志。使用传统IPC只能由用户在数据包里填入UID/PID，但这样不可靠，容易被恶意程序利用。可靠的身份标记只有由IPC机制本身在内核中添加。 其次传统IPC访问接入点是开放的，无法建立私有通道。比如命名管道的名称，system V的键值，socket的ip地址或文件名都是开放的，只要知道这些接入点的程序都可以和对端建立连接，不管怎样都无法阻止恶意程序通过猜测接收方地址获得连接。 关键点: 利用 UID 实现安全性 Binder 面向对象的设计思想 Binder使用Client-Server通信方式：一个进程作为Server提供诸如视频/音频解码，视频捕获，地址本查询，网络连接等服务；多个进程作为Client向Server发起服务请求，获得所需要的服务。 要想实现Client-Server通信据必须实现以下两点：一是server必须有确定的访问接入点或者说地址来接受Client的请求，并且Client可以通过某种途径获知Server的地址；二是制定Command-Reply协议来传输数据。例如在网络通信中Server的访问接入点就是Server主机的IP地址+端口号，传输协议为TCP协议。对Binder而言，Binder可以看成Server提供的实现某个特定服务的访问接入点， Client通过这个‘地址’向Server发送请求来使用该服务；对Client而言，Binder可以看成是通向Server的管道入口，要想和某个Server通信首先必须建立这个管道并获得管道入口。 与其它IPC不同，Binder使用了面向对象的思想来描述作为访问接入点的Binder及其在Client中的入口：Binder是一个实体位于Server中的对象，该对象提供了一套方法用以实现对服务的请求，就象类的成员函数。 遍布于client中的入口可以看成指向这个binder对象的‘指针’，一旦获得了这个‘指针’就可以调用该对象的方法访问server。在Client看来，通过Binder‘指针’调用其提供的方法和通过指针调用其它任何本地对象的方法并无区别，尽管前者的实体位于远端Server中，而后者实体位于本地内存中。‘指针’是C++的术语，而更通常的说法是引用，即Client通过Binder的引用访问Server。而软件领域另一个术语‘句柄’也可以用来表述Binder在Client中的存在方式。从通信的角度看，Client中的Binder也可以看作是Server Binder的‘代理’，在本地代表远端Server为Client提供服务。本文中会使用‘引用’或‘句柄’这个两广泛使用的术语。 面向对象思想的引入将进程间通信转化为通过对某个Binder对象的引用调用该对象的方法，而其独特之处在于Binder对象是一个可以跨进程引用的对象，它的实体位于一个进程中，而它的引用却遍布于系统的各个进程之中。 最诱人的是，这个引用和java里引用一样既可以是强类型，也可以是弱类型，而且可以从一个进程传给其它进程，让大家都能访问同一Server，就象将一个对象或引用赋值给另一个引用一样。Binder模糊了进程边界，淡化了进程间通信过程，整个系统仿佛运行于同一个面向对象的程序之中。形形色色的Binder对象以及星罗棋布的引用仿佛粘接各个应用程序的胶水，这也是Binder在英文里的原意。 当然面向对象只是针对应用程序而言，对于Binder驱动和内核其它模块一样使用C语言实现，没有类和对象的概念。Binder驱动为面向对象的进程间通信提供底层支持。 关键点：通过面向对象，把进程间通信转化成本地调用 Binder 架构 Binder框架定义了四个角色：Server，Client，ServiceManager 以及 Binder 驱动。其中 Server，Client，ServiceManager运行于用户空间，Binder 驱动运行于内核空间。 关键点：Server，Client，ServiceManager，Binder驱动 匿名 Binder 并不是所有Binder都需要注册给SMgr广而告之的。Server端可以通过已经建立的Binder连接将创建的Binder实体传给Client，当然这条已经建立的Binder连接必须是通过实名Binder实现。 由于这个Binder没有向SMgr注册名字，所以是个匿名Binder。Client将会收到这个匿名Binder的引用，通过这个引用向位于Server中的实体发送请求。匿名Binder为通信双方建立一条私密通道，只要Server没有把匿名Binder发给别的进程，别的进程就无法通过穷举或猜测等任何方式获得该Binder的引用，向该Binder发送请求 关键点：没有向 ServiceManager 注册名字 Binder CPP 类图 BpBinder: Binder proxy，表示代理，是客户端持有的一个代理对象transact：客户端持有后，BpBinder.transact() 用于发送命令。 BnBinder: Binder native，表示本地对象。与 BBinder 是同一个东西onTransact：本地服务端，BBinder.onTransact() 用于响应命令并处理。 BpRefBase mRemote 指针指向 IBinder，具体是 BpBinder 对象。 参考资料 https://redspider110.github.io/2017/12/21/0041-android-binder/ http://gityuan.com/2015/10/31/binder-prepare/ "},"doc/player/ExoPlayer.html":{"url":"doc/player/ExoPlayer.html","title":"Exoplayer","keywords":"","body":"Exoplayer "},"doc/player/NuPlayer.html":{"url":"doc/player/NuPlayer.html","title":"Nuplayer","keywords":"","body":"Nuplayer "},"doc/player/MediaPlayer.html":{"url":"doc/player/MediaPlayer.html","title":"Medialayer","keywords":"","body":"Medialayer "},"doc/player/VLC.html":{"url":"doc/player/VLC.html","title":"VLC","keywords":"","body":"VLC "},"doc/framework/FFmpeg.html":{"url":"doc/framework/FFmpeg.html","title":"FFmpeg","keywords":"","body":"FFmpeg 本篇文章目标 掌握 FFmpeg 命令的使用 掌握 FFmpeg 库的使用 了解 FFmpeg 架构 了解 FFmpeg 基本概念/术语 FFmpeg 简介 FFmpeg 项目试图为应用程序开发人员和最终用户提供技术上可能的最佳解决方案。FFmpeg 是领先的多媒体框架，能够解码、编码、转码、多路复用、多路分解、流式传输、过滤和播放人类和机器创建的几乎所有内容。用于处理音频、视频、字幕和相关元数据等多媒体内容的库和工具的集合。 它包含可供应用程序使用的 libavcodec、libavutil、libavformat、libavfilter、libavdevice、libswscale 和 libswresample。还有ffmpeg、ffplay、ffprobe，可供终端用户转码播放。 FFmpeg 命令行工具的使用 FFmpeg 有3个命令行工具：ffmpeg, ffplay, ffprobe ffmpeg: 用于操作、转换和流式传输多媒体内容的命令行工具箱。ffplay: 一个简约的多媒体播放器。ffprobe: 一个简单的分析工具来检查多媒体内容。 FFmpeg 库的使用 libavutil: 是一个包含用于简化编程的函数的库，包括随机数生成器、数据结构、数学例程、核心多媒体实用程序等等libavcodec: 是一个包含音频/视频编解码器的解码器和编码器的库。libavformat: 是一个包含多媒体容器格式的多路分解器和多路复用器的库。libavdevice: 是一个包含输入和输出设备的库，用于从许多常见的多媒体输入/输出软件框架（包括 Video4Linux、Video4Linux2、VfW 和 ALSA）中获取和渲染。libavfilter: 是一个包含媒体过滤器的库。libswscale: 是一个执行高度优化的图像缩放和色彩空间/像素格式转换操作的库。libswresample: 是一个执行高度优化的音频重采样、重新矩阵化和样本格式转换操作的库。 FFmpeg 架构 FFmpeg 应用 参考资料 https://blog.csdn.net/leixiaohua1020/article/details/15811977 https://ffmpeg.org/documentation.html "},"doc/framework/GSteamer.html":{"url":"doc/framework/GSteamer.html","title":"GSteamer","keywords":"","body":"GSteamer "},"doc/framework/OpenMax.html":{"url":"doc/framework/OpenMax.html","title":"OpenMax","keywords":"","body":"OpenMax 本篇文章目标 了解 OpenMax 框架 了解 OpenMax 的调用流程 了解 OpenMax 数据处理流程 了解 OpenMax component OpenMax 简介 OpenMax, 简称 OMX, 是一个多媒体应用程序的标准，是跨平台的 C 语言程序接口序列这些接口对音频、视频以及静态图片的常用操作进行封装 OpenMax 总共包括三层，分别是应用层(AI)、集成层(IL)和开发层(DL)。 应用层规定了应用程序和多媒体中间层的标准接口，使应用程序的移植性更好。 集成层定义了多媒体组件的接口，使得多媒体框架能以一种统一的方式访问多媒体 Codec 和组件，以便在嵌入式流媒体框架中快速集成加速编解码器。 开发层为 Codec 厂商和硬件厂商提供了一套 API，使开发更加便捷 在 Android 中，OpenMax 的 IL 层通常可以用于多媒体引擎的插件Android 的多媒体引擎 StageFright 可以使用 OpenMax 作为插件，主要用于编解码（Codec）的处理。 集成层(IL) OpenMax 集成层由 Client、Core、Component 和 Port 组成。 Client Client 通过 Core 得到对应 Component 的 Handle，而后通过命令直接和 Component 进行交互。 component 每个 Component 至少有一个 Port 进行数据交互如 Decoder 有一个输入 Port 接收码流，一个输出 Port 输出 YUV 序列。Component 内部可能通过消息处理机制完成 Client 要求的任务。 关键点：应用层、集成层、开发层、component OpenMax 在 Android 中的位置 OpenMax 框架 整体框架 IL 层框架 Component 框架 Component States OpenMax IL 头文件描述 OpenMax 标准只有头文件，没有标准的库。对于实现者，需要实现的主要是包含函数指针的结构体。头文件路径：frameworks/native/headers/media_plugin/media/openmax/ OpenMax 标准头文件的介绍如下：OpenMax_Types.h：OpenMax IL 的数据类型定义OpenMax_Core.h：OpenMax IL 核心的 APIOpenMax_Component.h：OpenMax IL 组件相关的 APIOpenMax_Audio.h：音频相关的常量和数据结构OpenMax_IVCommon.h：图像和视频公共的常量和数据结构OpenMax_Image.h：图像相关的常量和数据结构OpenMax_Video.h：视频相关的常量和数据结构OpenMax_Other.h：其他数据结构（包括 A/V 同步）OpenMax_Index.h：OpenMax IL 定义的数据结构索引OpenMax_ContentPipe.h：内容的管道定义 其中，OpenMax_Component.h 中定义的 OpenMax_COMPONENTTYPE 结构体是 OpenMax IL 层的核心内容。 OpenMax IL 层调用流程 OpenMax IL 层接口的调用流程如下： 1. OpenMax core 初始化OpenMax_Init：OpenMax 组件初始化，能力及端口查询，利用 OpenMax_GetHandle 调用来实例化想要的 OpenMax 组件，同时注册回调，利用 OpenMax_GetParameter 调用来获取 OpenMax 组件的能力和 OpenMax 组件上可用端口的数量 2. OpenMax 组件输入输出参数、及 buffer 设置OpenMax_GetParameter OpenMax_SetParameter OpenMax_UseBuffer / OpenMax_AllocateBuffer 3. OpenMax 状态转换（loaded -> idle -> executing）OpenMax_SendCommand 4. 数据注入、处理后的数据获取OpenMax_EmptyThisBuffer OpenMax_FillThisBuffer EmptyBufferDone FillBufferDone 5. OpenMax 状态转换（executing -> idle -> loaded）OpenMax_SendCommand 6. 释放 buffer 及 handleOpenMax_FreeBuffer OpenMax_FreeHandle 7. 反初始化OpenMax_Deinit 关键点：调用流程 OpenMax 服务启动流程图 media.codec 服务启动的时候，会创建 OpenMaxStore 和 OpenMax OpenMaxStore 的初始化OpenMaxStore 初始化会通过 MediaCodecsXmlParser 加载“/vendor/etc/media_codecs_ext.xml”、“/vendor/etc/media_codecs.xml”以及“/vendor/etc/media_codecs_performance.xml”文件，并保存解析到的音视频解码器描述信息。 OpenMax 的初始化OpenMax 的初始化会创建 OpenMaxMaster 以及 xml 文件解析类 MediaCodecsXmlParser。OpenMaxMaster 管理解码插件 plugin，其中包含 google 原生的 SoftOpenMaxPlugin 以及厂商的 VendorPlugin 产商的 VendorPlugin： 产商要实现自己的 OpenMax 解码插件，就必现定义一个继承了 OpenMaxPluginBase 基类的插件类，并实现基类的接口。 OpenMaxMaster 首先是加载产商插件 XxxOpenMaxPlugin，在 XxxOpenMaxPlugin 插件初始化的时候，会加载插件下面的各解码组件，其中包括视频硬解码组件、视频软解码组件以及音频软解码组件（包括 DTS 解码组件、AC3 解码组件和 ffmpeg 解码组件）。 Google 原生的 SoftOpenMaxPlugin： OpenMaxMaster 会去加载 google 原生的插件 SoftOpenMaxPlugin，在原生插件被加载的同时，插件内部也会加载 google 原生的各软解码组件。 OpenMaxMaster 加载完所有的插件和组件之后，会保存所有已成功加载的 plugin，以及各 component 和所属 plugin 的对应关系。 至此，整个 OpenMax 服务启动完成。 关键点：启动流程 OpenMax non-tunneled 数据 flow OpenMax tunneled 数据 flow OpenMax component 的工作方式 1. client 与 component 的通信 OpenMAX IL client 与 component 的通信有两种类型：一种是发送控制命令；另一种是发送数据处理消息。 两种通信的过程如下： 发送控制命令OpenMAX IL client 通过调用 OpenMax_SendCommand，来触发 messageSem 信号量，通知事件处理线程。 发送数据处理消息OpenMAX IL client 通过调用 OpenMax EmptyThisBuff 和 OpenMax FillThisBuff 来触发 bMgmtSem 信号量，通知数据处理线程，完成 component 的数据处理功能。 2. Component 线程 为了保证数据处理过程独立，Component 会创建两个线程： 一个是事件处理线程，构造此 component 时在 Constructor 中创建，用来处理用户发给 component 事件, 比如：OpenMax_CommandStateSet ：改变 component 状态OpenMax_CommandFlush ：清空 component 的数据队列OpenMax_CommandPortDisable ：禁止 component 的特定 portOpenMax_CommandPortEnable ：打开 Component 的特定 portOpenMax_CommandMarkBuffer ：申请用于数据交换的 buffer 另一个是数据处理线程，当 component 创建成功后，数据处理线程由 StateLoaded 状态转换到 StateIdle 状态时创建，用来对 component 的数据队列进行管理。 例如，对于一个 codec component 来说，主要完成如下功能： 从 input port 的数据队列取得一个输入 buffer，如果没有取到则等待，直到成功； 从 output port 的数据队列取得一个输出 buffer，如果没有取到则等待，直到成功； 调用 BufferMgmtCallback 函数进行解码； 解码完成后，触发 input port 和 output port 的 ReturnBufferFunction 函数，然后进入等待状态，等待下一轮处理过程。 关键点：IL client 与 component 的两种通信方式、component 的线程 OpenMax 相关 code 位置 frameworks/av/media/libstagefright/ACodec.cppframeworks/av/media/libstagefright/include/media/stagefright/ACodec.h frameworks/av/media/libstagefright/OpenMaxClient.cppframeworks/av/media/libstagefright/include/media/stagefright/OpenMaxClient.h frameworks/av/media/libstagefright/OpenMax/frameworks/av/media/libstagefright/include/media/stagefright/ frameworks/av/media/libstagefright/OpenMax/ OpenMaxMaster.cppframeworks/av/media/libstagefright/OpenMax/include/media/stagefright/OpenMax/ OpenMaxMaster.h frameworks/av/media/libstagefright/OpenMax/SoftOpenMaxPlugin.cppframeworks/av/media/libstagefright/OpenMax/include/media/stagefright/OpenMax/SoftOpenMaxPlugin.h frameworks/av/media/libstagefright/OpenMax/OpenMaxNodeInstance.cppframeworks/av/media/libstagefright/OpenMax/include/media/stagefright/OpenMax/OpenMaxNodeInstance.h frameworks/av/media/libstagefright/OpenMax/SoftOpenMaxComponent.cppframeworks/av/media/libstagefright/OpenMax/include/media/stagefright/OpenMax/SoftOpenMaxComponent.h frameworks/av/media/libstagefright/OpenMax/SimpleSoftOpenMaxComponent.cppframeworks/av/media/libstagefright/OpenMax/include/media/stagefright/OpenMax/SimpleSoftOpenMaxComponent.h frameworks/av/media/libstagefright/codecs frameworks/av/media/libstagefright/codecs/aacdec/SoftAAC2.cppframeworks/av/media/libstagefright/codecs/aacdec/SoftAAC2.h OpenMax AAC/MP3 decode flow 具体 flow 可参考如下资料：Android 平台 aac 谷歌软解框架和流程、解码库学习OpenMax Codec 详细解析 参考资料 OpenMax官网OpenMax IL 官方文档OpenMax 服务启动Android OpenMax 介绍Android MultiMedia 框架完全解析Android 平台 aac 谷歌软解框架和流程、解码库学习OpenMax Codec 详细解析Android 解码器 component 加载 "},"doc/framework/Codec2.html":{"url":"doc/framework/Codec2.html","title":"Codec2","keywords":"","body":"Codec2 "},"doc/framework/MediaCodec.html":{"url":"doc/framework/MediaCodec.html","title":"MediaCodec","keywords":"","body":"MediaCodec 本篇文章目标 了解 MediaCodec 掌握 MediaCodec API 使用流程 掌握 MediaCodec API MediaCodec 基本介绍 MediaCodec 类可用于访问低级媒体编解码器，即编码器/解码器组件。MediaCodec 是 Android 提供的用于对音视频进行编解码的类，它通过访问底层的 codec 来实现编解码的功能它是 Android 低级多媒体支持基础结构的一部分它为芯片厂商和应用开发者搭建了一个统一接口 MediaCodec 在 Android 中的位置 MediaCodec 的生命周期 MediaCodec 数据处理流程设计 从上图可以看出 MediaCodec 架构上采用了 2 个缓冲区队列，异步处理 input 和 output 数据，并且使用了一组输入输出缓存。 Client 请求或接收到一个空的输入缓存（input buffer），向其中填充满数据并将它传递给编解码器处理。编解码器处理完这些数据并将处理结果输出至一个空的输出缓存（output buffer）中。最终，Client 请求或接收到一个填充了结果数据的输出缓存（output buffer），使用完其中的数据，并将其释放给编解码器再次使用。 具体工作如下： Client 从 input 缓冲区队列申请 empty buffer [dequeueInputBuffer] Client 把需要编解码的数据拷贝到 empty buffer，然后放入 input 缓冲区队列 [queueInputBuffer] MediaCodec 模块从 input 缓冲区队列取一帧数据进行编解码处理 编解码处理结束后，MediaCodec 将原始数据 buffer 置为 empty 后放回 input 缓冲区队列，将编解码后的数据放入到 output 缓冲区队列 Client 从 output 缓冲区队列申请编解码后的 buffer [dequeueOutputBuffer] Client 对编解码后的 buffer 进行渲染/播放 渲染/播放完成后，Client 再将该 buffer 放回 output 缓冲区队列 [releaseOutputBuffer] MediaCodec 的基本调用流程 同步模式调用流程 (synchronous mode) MediaCodec codec = MediaCodec.createByCodecName(name); codec.configure(format, …); MediaFormat outputFormat = codec.getOutputFormat(); // option B codec.start(); for (;;) { int inputBufferId = codec.dequeueInputBuffer(timeoutUs); if (inputBufferId >= 0) { ByteBuffer inputBuffer = codec.getInputBuffer(…); // fill inputBuffer with valid data … codec.queueInputBuffer(inputBufferId, …); } int outputBufferId = codec.dequeueOutputBuffer(…); if (outputBufferId >= 0) { ByteBuffer outputBuffer = codec.getOutputBuffer(outputBufferId); MediaFormat bufferFormat = codec.getOutputFormat(outputBufferId); // option A // bufferFormat is identical to outputFormat // outputBuffer is ready to be processed or rendered. … codec.releaseOutputBuffer(outputBufferId, …); } else if (outputBufferId == MediaCodec.INFO_OUTPUT_FORMAT_CHANGED) { // Subsequent data will conform to new format. // Can ignore if using getOutputFormat(outputBufferId) outputFormat = codec.getOutputFormat(); // option B } } codec.stop(); codec.release(); 异步模式调用流程(asynchronous mode) MediaCodec codec = MediaCodec.createByCodecName(name); MediaFormat mOutputFormat; // member variable codec.setCallback(new MediaCodec.Callback() { @Override void onInputBufferAvailable(MediaCodec mc, int inputBufferId) { ByteBuffer inputBuffer = codec.getInputBuffer(inputBufferId); // fill inputBuffer with valid data … codec.queueInputBuffer(inputBufferId, …); } @Override void onOutputBufferAvailable(MediaCodec mc, int outputBufferId, …) { ByteBuffer outputBuffer = codec.getOutputBuffer(outputBufferId); MediaFormat bufferFormat = codec.getOutputFormat(outputBufferId); // option A // bufferFormat is equivalent to mOutputFormat // outputBuffer is ready to be processed or rendered. … codec.releaseOutputBuffer(outputBufferId, …); } @Override void onOutputFormatChanged(MediaCodec mc, MediaFormat format) { // Subsequent data will conform to new format. // Can ignore if using getOutputFormat(outputBufferId) mOutputFormat = format; // option B } @Override void onError(…) { … } }); codec.configure(format, …); mOutputFormat = codec.getOutputFormat(); // option B codec.start(); // wait for processing to complete codec.stop(); codec.release(); MediaCodec API 介绍 https://developer.android.com/reference/android/media/MediaCodec#public-methods_1 参考资料 https://developer.android.com/reference/android/media/MediaCodec "},"doc/framework/V4L2.html":{"url":"doc/framework/V4L2.html","title":"V4L2","keywords":"","body":"V4L2 "},"doc/framework/ALSA.html":{"url":"doc/framework/ALSA.html","title":"ALSA","keywords":"","body":"ALSA "},"doc/framework/TinyALSA.html":{"url":"doc/framework/TinyALSA.html","title":"TinyALSA","keywords":"","body":"TinyALSA 本篇文章目标 了解 TinyALSA 掌握 TinyALSA 命令行工具的使用 掌握 TinyALSA API 的使用 TinyALSA 基本介绍 TinyALSA 是一个适用于 Linux 系统的 PCM 音频小型库。它提供了一个简单轻量的 API，用于访问和控制 Linux 上的音频设备。TinyALSA 的设计目的是易于使用和理解，因此是需要基本 PCM 音频功能的项目的好选择。 目标是： 提供基本的 pcm 和 mixer API。 如果不是绝对需要，请不要将其添加到 API 中。 避免支持可以在更高级别处理的复杂和不必要的操作。 提供全面的文档。 TinyALSA 位于 Android 源码的 external/TinyALSA 位置 关键点：PCM, API, 轻量 TinyALSA 命令行工具 TinyALSA 提供了如下命令行工具: tinyplay, tinycap, tinymix, tinypcminfo tinyplay：用于播放音频数据，它可以用于播放音频文件（wav 或原始音频样本）到音频设备 tinycap：用于录制音频数据，它可以从音频设备读取数据，并将数据写入 WAV 文件或标准输出（作为原始样本） tinymix：用于控制音频设备，它可以用来控制音量、混音器等 tinypcminfo：用于显示有关音频设备的信息。它可以用来检查音频硬件的可用性，并显示关于该硬件的详细信息，例如支持的音频格式，最大播放速率等。 关键点：tinyplay, tinycap, tinymix, tinypcminfo tinyplay 用法 tinyplay file [ options ] 其中 file 表示需要播放的音频文件，options 表示指定音频设备和文件参数的选项。 选项 -D, --card card：音频设备的卡编号。默认为 0。-d, --device device：音频设备的设备编号。默认为 0。-c, --channels channels：音频设备的通道数。仅对原始音频文件有效。默认为 2。-r, --rate rate：每秒的帧数。仅对原始音频文件有效。默认为 48000。-i, --file-type file-type：用于播放的文件类型。可用类型为 raw 和 wav。指定 raw 意味着通道数、帧率和比特数也可能需要指定。默认情况下，文件类型由文件名确定。使用该选项指定的文件类型优先于由文件名确定的文件类型。-b, --bits bits：每个样本的比特数。仅对原始音频文件有效。默认为 16。 -p, --period-size period_size: PCM 中一个周期的帧数。默认值为 1024。-n, --period-count periods: PCM 中的周期数。默认值为 4。 例子 播放名为 output.wav 的音频文件 tinyplay output.wav 在声卡 1 上播放名为 output.wav 的音频文件 tinyplay output.wav -D 1 使用 2 个声道、44100 每秒帧数和每采样 32 位播放名为 output.raw 的原始音频文件 tinyplay output.raw -i raw --channels 2 --rate 44100 --bits 32 tinycap 用法 tinycap [ file ] [ options ] 其中 file 表示录音生成的文件，options 表示指定音频设备和文件参数的选项。 选项 -D, --card card：音频设备的卡编号。默认为 0。-d, --device device：音频设备的设备编号。默认为 0。-M：使用内存映射 I/O 方法。如果不指定此选项，则使用读写 I/O 方法。-c, --channels channels：音频设备的通道数。仅对原始音频文件有效。默认为 2。-r, --rate rate：每秒的帧数。仅对原始音频文件有效。默认为 48000。-i, --file-type file-type：用于播放的文件类型。可用类型为 raw 和 wav。指定 raw 意味着通道数、帧率和比特数也可能需要指定。默认情况下，文件类型由文件名确定。使用该选项指定的文件类型优先于由文件名确定的文件类型。-b, --bits bits：每个样本的比特数。仅对原始音频文件有效。默认为 16。-p, --period-size period_size: PCM 中一个周期的帧数。默认值为 1024。-n, --period-count periods: PCM 中的周期数。默认值为 4。 -t seconds：录制音频的秒数。 例子 录制名为 output.wav 的文件，直到捕获中断信号 tinycap output.wav 录制名为 output.wav 的文件，从第 1 个声卡上录制两秒音频或直到捕获中断信号。 tinycap output.wav -D 1 -t 2 录制到标准输出，录制三秒音频或直到捕获中断信号。 tinycap -- -t 3 tinymix 用法 tinymix [ options ] command options 选项 -D, --card card：混音器卡号，默认值为 0。-h, --help：打印帮助内容并退出。-v, --version：打印当前 tinymix 版本并退出。 command 输出指定控件的值 get 设置指定控件的值 set 输出所有混音器控件的内容 contents 输出所有混音器控件的名称和 ID controls 例子 输出编号为 0 的混音器的控件 ID 列表。 tinymix controls 输出编号为 1 的混音器的控件 ID 列表。 tinymix -D 1 controls 输出编号为 0 的控件的信息。 tinymix get 0 输出名称为“Headphone Playback Volume”的控件的信息。 tinymix get \"Headphone Playback Volume\" 将控件 0 的值设置为 4。 tinymix set 0 4 将编号为 1 的混音器的控件 2 的值设置为 32。 tinymix --card 1 set 2 32 tinypcminfo 用法 tinypcminfo [ options ] 选项 -D card: 指定 PCM 所在的卡号。默认为 0。-d device：指定 PCM 所在的设备号。默认为 0。 例子 输出卡号为 1，设备号为 1 的 PCM 的硬件参数。 tinypcminfo -D 1 -d 1 TinyALSA API 参考资料 https://github.com/tinyalsa/tinyalsa "},"doc/codec/video/H264.html":{"url":"doc/codec/video/H264.html","title":"H264","keywords":"","body":"H264 "},"doc/codec/video/H265.html":{"url":"doc/codec/video/H265.html","title":"H265","keywords":"","body":"H265 "},"doc/codec/audio/AAC.html":{"url":"doc/codec/audio/AAC.html","title":"AAC","keywords":"","body":"AAC "},"doc/codec/audio/Opus.html":{"url":"doc/codec/audio/Opus.html","title":"Opus","keywords":"","body":"Opus "},"doc/transfer_protocol/WebRTC.html":{"url":"doc/transfer_protocol/WebRTC.html","title":"WebRTC","keywords":"","body":"WebRTC "},"doc/transfer_protocol/RTP.html":{"url":"doc/transfer_protocol/RTP.html","title":"RTP","keywords":"","body":"RTP "},"doc/transfer_protocol/RTCP.html":{"url":"doc/transfer_protocol/RTCP.html","title":"RTCP","keywords":"","body":"RTCP "},"doc/transfer_protocol/RTMP.html":{"url":"doc/transfer_protocol/RTMP.html","title":"RTMP","keywords":"","body":"RTMP "},"doc/transfer_protocol/RTSP.html":{"url":"doc/transfer_protocol/RTSP.html","title":"RTSP","keywords":"","body":"RTSP "},"doc/transfer_protocol/HLS.html":{"url":"doc/transfer_protocol/HLS.html","title":"HLS","keywords":"","body":"HLS "},"doc/transfer_protocol/HTTP.html":{"url":"doc/transfer_protocol/HTTP.html","title":"HTTP","keywords":"","body":"HTTP "},"doc/transfer_protocol/TCP.html":{"url":"doc/transfer_protocol/TCP.html","title":"TCP","keywords":"","body":"TCP "},"doc/transfer_protocol/IGMP.html":{"url":"doc/transfer_protocol/IGMP.html","title":"IGMP","keywords":"","body":"IGMP "},"doc/cast_screen/DLNA.html":{"url":"doc/cast_screen/DLNA.html","title":"DLNA","keywords":"","body":"DLNA "},"doc/cast_screen/Miracast.html":{"url":"doc/cast_screen/Miracast.html","title":"Miracast","keywords":"","body":"Miracast "},"doc/cast_screen/AirPlay.html":{"url":"doc/cast_screen/AirPlay.html","title":"AirPlay","keywords":"","body":"AirPlay "},"doc/other/Git.html":{"url":"doc/other/Git.html","title":"Git","keywords":"","body":"Git "},"doc/other/Gerrit.html":{"url":"doc/other/Gerrit.html","title":"Gerrit","keywords":"","body":"Gerrit "},"doc/other/Jinken.html":{"url":"doc/other/Jinken.html","title":"Jinkens","keywords":"","body":"Jinkens "},"doc/other/OpenGrok.html":{"url":"doc/other/OpenGrok.html","title":"OpenGrok","keywords":"","body":"OpenGrok "},"doc/other/RDK.html":{"url":"doc/other/RDK.html","title":"RDK","keywords":"","body":"RDK "},"doc/other/Yocto.html":{"url":"doc/other/Yocto.html","title":"Yocto","keywords":"","body":"Yocto "},"doc/other/BuildRoot.html":{"url":"doc/other/BuildRoot.html","title":"BuildRoot","keywords":"","body":"BuildRoot "},"doc/other/SysTrace.html":{"url":"doc/other/SysTrace.html","title":"SysTrace","keywords":"","body":"SysTrace "},"doc/other/DumpSys.html":{"url":"doc/other/DumpSys.html","title":"DumpSys","keywords":"","body":"DumpSys "},"doc/other/Ninja.html":{"url":"doc/other/Ninja.html","title":"Ninja","keywords":"","body":"Ninja "},"doc/other/BugReport.html":{"url":"doc/other/BugReport.html","title":"BugReport","keywords":"","body":"BugReport "}}